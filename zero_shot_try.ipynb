{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd07d9d8",
   "metadata": {},
   "source": [
    "# Zero-Shot Anomaly Detection using DINOv3\n",
    "This notebook implements a full pipeline for zero-shot anomaly detection using DINO features in an object-oriented way. It covers preprocessing, feature extraction, feature bank creation, scoring, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3b3b1",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "Imports all required libraries and sets up the device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a20382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import faiss\n",
    "from dataclasses import dataclass\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5445d",
   "metadata": {},
   "source": [
    "## 2. ImagePreprocessor Class\n",
    "Handles image loading, resizing, normalization, and batching for DINO input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71e9343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ImagePreprocessor class defined\n"
     ]
    }
   ],
   "source": [
    "class ImagePreprocessor:\n",
    "    def __init__(self, input_size: int = 224, batch_size: int = 32):\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(int(input_size * 1.14)),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def load_image(self, image_path: str) -> Image.Image:\n",
    "        if isinstance(image_path, str):\n",
    "            return Image.open(image_path).convert('RGB')\n",
    "        return image_path\n",
    "    def preprocess_single(self, image: Image.Image) -> torch.Tensor:\n",
    "        return self.transform(image).unsqueeze(0)\n",
    "    def preprocess_batch(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        tensors = [self.transform(img) for img in images]\n",
    "        return torch.stack(tensors)\n",
    "    def load_images_from_directory(self, directory: str, extensions: Tuple[str, ...] = ('.png', '.jpg', '.jpeg')) -> List[str]:\n",
    "        directory_path = Path(directory)\n",
    "        image_paths = []\n",
    "        for ext in extensions:\n",
    "            image_paths.extend(directory_path.glob(f\"*{ext}\"))\n",
    "        return sorted([str(p) for p in image_paths])\n",
    "    def create_batches(self, image_paths: List[str]) -> List[Tuple[torch.Tensor, List[str]]]:\n",
    "        batches = []\n",
    "        for i in range(0, len(image_paths), self.batch_size):\n",
    "            batch_paths = image_paths[i:i + self.batch_size]\n",
    "            batch_images = [self.load_image(path) for path in batch_paths]\n",
    "            batch_tensor = self.preprocess_batch(batch_images)\n",
    "            batches.append((batch_tensor, batch_paths))\n",
    "        return batches\n",
    "print(\"✓ ImagePreprocessor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013752df",
   "metadata": {},
   "source": [
    "## 3. FeatureExtractor Class\n",
    "Extracts patch-level features from DINOv3 models, handling both tensor and dict outputs. L2-normalizes features if specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c06a464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ FeatureExtractor class defined\n"
     ]
    }
   ],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, model, device: torch.device, l2_normalize: bool = True):\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.l2_normalize = l2_normalize\n",
    "        self.patch_size = 16\n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, images: torch.Tensor, return_cls: bool = False) -> torch.Tensor:\n",
    "        images = images.to(self.device)\n",
    "        if hasattr(self.model, 'forward_features'):\n",
    "            features = self.model.forward_features(images)\n",
    "            if isinstance(features, dict):\n",
    "                if 'x_norm_patchtokens' in features:\n",
    "                    patch_features = features['x_norm_patchtokens']\n",
    "                    cls_features = features.get('x_norm_clstoken', None)\n",
    "                elif 'x_prenorm' in features:\n",
    "                    all_tokens = features['x_prenorm']\n",
    "                    cls_features = all_tokens[:, 0, :]\n",
    "                    patch_features = all_tokens[:, 1:, :]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected dict keys: {features.keys()}\")\n",
    "            else:\n",
    "                if len(features.shape) == 3:\n",
    "                    cls_features = features[:, 0, :]\n",
    "                    patch_features = features[:, 1:, :]\n",
    "                elif len(features.shape) == 2:\n",
    "                    features = self.model.get_intermediate_layers(images, n=1, return_class_token=True)\n",
    "                    if isinstance(features, (list, tuple)):\n",
    "                        features = features[0]\n",
    "                    cls_features = features[0] if isinstance(features, (list, tuple)) else features[:, 0, :]\n",
    "                    patch_features = features[1] if isinstance(features, (list, tuple)) else features[:, 1:, :]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected feature shape: {features.shape}\")\n",
    "        else:\n",
    "            features = self.model(images)\n",
    "            if isinstance(features, dict):\n",
    "                if 'x_norm_patchtokens' in features:\n",
    "                    patch_features = features['x_norm_patchtokens']\n",
    "                    cls_features = features.get('x_norm_clstoken', None)\n",
    "                else:\n",
    "                    raise ValueError(f\"Cannot extract patches from dict: {features.keys()}\")\n",
    "            elif len(features.shape) == 2:\n",
    "                patch_features = self.model.get_intermediate_layers(images, n=1, return_class_token=False)[0]\n",
    "                cls_features = features\n",
    "            else:\n",
    "                cls_features = features[:, 0, :]\n",
    "                patch_features = features[:, 1:, :]\n",
    "        if self.l2_normalize:\n",
    "            patch_features = F.normalize(patch_features, p=2, dim=-1)\n",
    "            if return_cls and cls_features is not None:\n",
    "                cls_features = F.normalize(cls_features, p=2, dim=-1)\n",
    "        if return_cls:\n",
    "            return patch_features, cls_features\n",
    "        return patch_features\n",
    "    def get_feature_dim(self) -> int:\n",
    "        if hasattr(self.model, 'embed_dim'):\n",
    "            return self.model.embed_dim\n",
    "        elif hasattr(self.model, 'num_features'):\n",
    "            return self.model.num_features\n",
    "        elif hasattr(self.model, 'backbone'):\n",
    "            if hasattr(self.model.backbone, 'embed_dim'):\n",
    "                return self.model.backbone.embed_dim\n",
    "        return 384\n",
    "    def get_patch_grid_size(self, input_size: int = 224) -> Tuple[int, int]:\n",
    "        grid_h = grid_w = input_size // self.patch_size\n",
    "        return (grid_h, grid_w)\n",
    "    def extract_from_multiple_batches(self, batches: List[Tuple[torch.Tensor, List[str]]]) -> Dict[str, np.ndarray]:\n",
    "        feature_dict = {}\n",
    "        for batch_tensor, batch_paths in tqdm(batches, desc=\"Extracting features\"):\n",
    "            patch_features = self.extract_features(batch_tensor)\n",
    "            patch_features_np = patch_features.cpu().numpy()\n",
    "            for i, path in enumerate(batch_paths):\n",
    "                feature_dict[path] = patch_features_np[i]\n",
    "        return feature_dict\n",
    "print(\"✓ FeatureExtractor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b012c4",
   "metadata": {},
   "source": [
    "## 4. NormalFeatureBank Class\n",
    "Stores and indexes normal image features using FAISS for fast k-NN search and computes Mahalanobis statistics. If you want to use GPU acceleration for FAISS, set `use_gpu_faiss=True` and ensure you have `faiss-gpu` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2ff6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NormalFeatureBank class defined\n"
     ]
    }
   ],
   "source": [
    "class NormalFeatureBank:\n",
    "    def __init__(self, feature_dim: int, use_gpu: bool = True, index_type: str = 'flat'):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.index_type = index_type\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available() and hasattr(faiss, 'StandardGpuResources')\n",
    "        if use_gpu and not self.use_gpu:\n",
    "            print(\"Warning: GPU FAISS requested but not available. Using CPU version.\")\n",
    "            print(\"To use GPU FAISS, install with: pip uninstall faiss-cpu && pip install faiss-gpu\")\n",
    "        self.index = self._create_index()\n",
    "        self.mean = None\n",
    "        self.cov_inv = None\n",
    "        self.features_array = None\n",
    "    def _create_index(self) -> faiss.Index:\n",
    "        if self.index_type == 'flat':\n",
    "            index = faiss.IndexFlatL2(self.feature_dim)\n",
    "        elif self.index_type == 'ivf':\n",
    "            quantizer = faiss.IndexFlatL2(self.feature_dim)\n",
    "            index = faiss.IndexIVFFlat(quantizer, self.feature_dim, 100)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown index type: {self.index_type}\")\n",
    "        if self.use_gpu:\n",
    "            try:\n",
    "                res = faiss.StandardGpuResources()\n",
    "                index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "                print(\"✓ Using GPU for FAISS index\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to move index to GPU: {e}\")\n",
    "                print(\"Continuing with CPU index\")\n",
    "                self.use_gpu = False\n",
    "        else:\n",
    "            print(\"✓ Using CPU for FAISS index\")\n",
    "        return index\n",
    "    def build_from_features(self, feature_dict: Dict[str, np.ndarray], compute_statistics: bool = True):\n",
    "        all_features = []\n",
    "        for features in feature_dict.values():\n",
    "            all_features.append(features)\n",
    "        self.features_array = np.vstack(all_features).astype('float32')\n",
    "        print(f\"Building index with {self.features_array.shape[0]} patch features...\")\n",
    "        if self.index_type == 'ivf' and not self.index.is_trained:\n",
    "            self.index.train(self.features_array)\n",
    "        self.index.add(self.features_array)\n",
    "        if compute_statistics:\n",
    "            self._compute_statistics()\n",
    "        print(f\"✓ Feature bank built with {self.index.ntotal} features\")\n",
    "    def _compute_statistics(self):\n",
    "        print(\"Computing Mahalanobis statistics...\")\n",
    "        self.mean = np.mean(self.features_array, axis=0)\n",
    "        centered = self.features_array - self.mean\n",
    "        cov = np.cov(centered.T)\n",
    "        reg = 1e-5\n",
    "        cov = cov + reg * np.eye(self.feature_dim)\n",
    "        try:\n",
    "            self.cov_inv = np.linalg.inv(cov)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Warning: Covariance matrix is singular, using pseudo-inverse\")\n",
    "            self.cov_inv = np.linalg.pinv(cov)\n",
    "        print(\"✓ Statistics computed\")\n",
    "    def search_knn(self, query_features: np.ndarray, k: int = 1) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        query_features = query_features.astype('float32')\n",
    "        distances, indices = self.index.search(query_features, k)\n",
    "        return distances, indices\n",
    "    def compute_mahalanobis_distance(self, query_features: np.ndarray) -> np.ndarray:\n",
    "        if self.mean is None or self.cov_inv is None:\n",
    "            raise ValueError(\"Statistics not computed. Set compute_statistics=True when building.\")\n",
    "        centered = query_features - self.mean\n",
    "        mahal = np.sum(centered @ self.cov_inv * centered, axis=1)\n",
    "        return np.sqrt(mahal)\n",
    "    def save(self, path: str):\n",
    "        save_dict = {\n",
    "            'features_array': self.features_array,\n",
    "            'mean': self.mean,\n",
    "            'cov_inv': self.cov_inv,\n",
    "            'feature_dim': self.feature_dim,\n",
    "            'index_type': self.index_type\n",
    "        }\n",
    "        index_path = path + '.index'\n",
    "        if self.use_gpu:\n",
    "            try:\n",
    "                cpu_index = faiss.index_gpu_to_cpu(self.index)\n",
    "                faiss.write_index(cpu_index, index_path)\n",
    "            except:\n",
    "                print(\"Warning: Could not convert GPU index to CPU. Saving as-is.\")\n",
    "                faiss.write_index(self.index, index_path)\n",
    "        else:\n",
    "            faiss.write_index(self.index, index_path)\n",
    "        np.savez(path + '.npz', **save_dict)\n",
    "        print(f\"✓ Feature bank saved to {path}\")\n",
    "    def load(self, path: str):\n",
    "        data = np.load(path + '.npz', allow_pickle=True)\n",
    "        self.features_array = data['features_array']\n",
    "        self.mean = data['mean']\n",
    "        self.cov_inv = data['cov_inv']\n",
    "        self.feature_dim = int(data['feature_dim'])\n",
    "        self.index_type = str(data['index_type'])\n",
    "        index_path = path + '.index'\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        if self.use_gpu:\n",
    "            try:\n",
    "                res = faiss.StandardGpuResources()\n",
    "                self.index = faiss.index_cpu_to_gpu(res, 0, self.index)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not move loaded index to GPU: {e}\")\n",
    "                self.use_gpu = False\n",
    "        print(f\"✓ Feature bank loaded from {path}\")\n",
    "print(\"✓ NormalFeatureBank class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d236868",
   "metadata": {},
   "source": [
    "## 5. AnomalyScorer Class\n",
    "Computes patch-level and image-level anomaly scores using k-NN or Mahalanobis distance. Also provides visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbeced2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AnomalyScorer class defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AnomalyResult:\n",
    "    image_score: float\n",
    "    patch_scores: np.ndarray\n",
    "    anomaly_map: np.ndarray\n",
    "    is_anomaly: bool\n",
    "    method: str\n",
    "class AnomalyScorer:\n",
    "    def __init__(self, feature_bank: NormalFeatureBank, patch_grid_size: Tuple[int, int], method: str = 'knn', k: int = 1, threshold: Optional[float] = None):\n",
    "        self.feature_bank = feature_bank\n",
    "        self.patch_grid_size = patch_grid_size\n",
    "        self.method = method\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "    def compute_patch_scores(self, patch_features: np.ndarray) -> np.ndarray:\n",
    "        if self.method == 'knn':\n",
    "            distances, _ = self.feature_bank.search_knn(patch_features, k=self.k)\n",
    "            scores = np.mean(distances, axis=1)\n",
    "        elif self.method == 'mahalanobis':\n",
    "            scores = self.feature_bank.compute_mahalanobis_distance(patch_features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "        return scores\n",
    "    def create_anomaly_map(self, patch_scores: np.ndarray, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:\n",
    "        H, W = self.patch_grid_size\n",
    "        score_map = patch_scores.reshape(H, W)\n",
    "        anomaly_map = cv2.resize(score_map, target_size, interpolation=cv2.INTER_CUBIC)\n",
    "        return anomaly_map\n",
    "    def compute_image_score(self, patch_scores: np.ndarray, aggregation: str = 'max') -> float:\n",
    "        if aggregation == 'max':\n",
    "            return float(np.max(patch_scores))\n",
    "        elif aggregation == 'mean':\n",
    "            return float(np.mean(patch_scores))\n",
    "        elif aggregation == 'percentile_95':\n",
    "            return float(np.percentile(patch_scores, 95))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation: {aggregation}\")\n",
    "    def score_image(self, patch_features: np.ndarray, target_size: Tuple[int, int] = (224, 224), aggregation: str = 'max') -> AnomalyResult:\n",
    "        patch_scores = self.compute_patch_scores(patch_features)\n",
    "        image_score = self.compute_image_score(patch_scores, aggregation)\n",
    "        anomaly_map = self.create_anomaly_map(patch_scores, target_size)\n",
    "        is_anomaly = False\n",
    "        if self.threshold is not None:\n",
    "            is_anomaly = image_score > self.threshold\n",
    "        return AnomalyResult(image_score=image_score, patch_scores=patch_scores, anomaly_map=anomaly_map, is_anomaly=is_anomaly, method=self.method)\n",
    "    def set_threshold_from_normal_scores(self, normal_image_scores: List[float], percentile: float = 99.0):\n",
    "        self.threshold = np.percentile(normal_image_scores, percentile)\n",
    "        print(f\"✓ Threshold set to {self.threshold:.4f} ({percentile}th percentile)\")\n",
    "    def visualize_result(self, image: Image.Image, result: AnomalyResult, figsize: Tuple[int, int] = (15, 5)):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis('off')\n",
    "        im = axes[1].imshow(result.anomaly_map, cmap='jet')\n",
    "        axes[1].set_title(f\"Anomaly Map ({result.method})\")\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[1])\n",
    "        axes[2].imshow(image)\n",
    "        axes[2].imshow(result.anomaly_map, cmap='jet', alpha=0.5)\n",
    "        status = \"ANOMALY\" if result.is_anomaly else \"NORMAL\"\n",
    "        axes[2].set_title(f\"Overlay - Score: {result.image_score:.4f} [{status}]\")\n",
    "        axes[2].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "print(\"✓ AnomalyScorer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b11a8d",
   "metadata": {},
   "source": [
    "## 6. ZeroShotAnomalyDetector Class\n",
    "Main orchestrator that integrates all components for end-to-end anomaly detection. Adjust parameters such as `input_size`, `batch_size`, `l2_normalize`, `use_gpu_faiss`, `index_type`, `scoring_method`, and `k` as needed for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36991b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ZeroShotAnomalyDetector class defined\n"
     ]
    }
   ],
   "source": [
    "class ZeroShotAnomalyDetector:\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 input_size: int = 224,\n",
    "                 batch_size: int = 32,\n",
    "                 device: torch.device = None,\n",
    "                 l2_normalize: bool = True,\n",
    "                 use_gpu_faiss: bool = True,\n",
    "                 index_type: str = 'flat',\n",
    "                 scoring_method: str = 'knn',\n",
    "                 k: int = 1):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preprocessor = ImagePreprocessor(input_size=input_size, batch_size=batch_size)\n",
    "        self.feature_extractor = FeatureExtractor(model, device=self.device, l2_normalize=l2_normalize)\n",
    "        self.input_size = input_size\n",
    "        self.scoring_method = scoring_method\n",
    "        self.k = k\n",
    "        self.use_gpu_faiss = use_gpu_faiss\n",
    "        self.index_type = index_type\n",
    "        self.feature_bank = None\n",
    "        self.scorer = None\n",
    "        print(f\"✓ ZeroShotAnomalyDetector initialized on {self.device}\")\n",
    "    def fit(self, normal_images_dir: str, compute_mahalanobis: bool = True):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"TRAINING: Building normal feature bank\")\n",
    "        print(f\"{'='*60}\")\n",
    "        image_paths = self.preprocessor.load_images_from_directory(normal_images_dir)\n",
    "        print(f\"Found {len(image_paths)} normal images\")\n",
    "        batches = self.preprocessor.create_batches(image_paths)\n",
    "        print(f\"Created {len(batches)} batches\")\n",
    "        feature_dict = self.feature_extractor.extract_from_multiple_batches(batches)\n",
    "        feature_dim = self.feature_extractor.get_feature_dim()\n",
    "        self.feature_bank = NormalFeatureBank(\n",
    "            feature_dim=feature_dim,\n",
    "            use_gpu=self.use_gpu_faiss,\n",
    "            index_type=self.index_type\n",
    "        )\n",
    "        compute_stats = compute_mahalanobis or (self.scoring_method == 'mahalanobis')\n",
    "        self.feature_bank.build_from_features(feature_dict, compute_statistics=compute_stats)\n",
    "        patch_grid_size = self.feature_extractor.get_patch_grid_size(self.input_size)\n",
    "        self.scorer = AnomalyScorer(\n",
    "            feature_bank=self.feature_bank,\n",
    "            patch_grid_size=patch_grid_size,\n",
    "            method=self.scoring_method,\n",
    "            k=self.k\n",
    "        )\n",
    "        print(f\"\\n✓ Training complete!\")\n",
    "        return self\n",
    "    def set_threshold(self, validation_images_dir: str, percentile: float = 99.0):\n",
    "        if self.scorer is None:\n",
    "            raise ValueError(\"Detector not trained. Call fit() first.\")\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"CALIBRATION: Setting threshold from validation set\")\n",
    "        print(f\"{'='*60}\")\n",
    "        validation_scores = []\n",
    "        image_paths = self.preprocessor.load_images_from_directory(validation_images_dir)\n",
    "        for img_path in tqdm(image_paths, desc=\"Scoring validation images\"):\n",
    "            image = self.preprocessor.load_image(img_path)\n",
    "            result = self.predict(image)\n",
    "            validation_scores.append(result.image_score)\n",
    "        self.scorer.set_threshold_from_normal_scores(validation_scores, percentile)\n",
    "        print(f\"Validation score range: [{min(validation_scores):.4f}, {max(validation_scores):.4f}]\")\n",
    "        print(f\"Mean: {np.mean(validation_scores):.4f}, Std: {np.std(validation_scores):.4f}\")\n",
    "        return self\n",
    "    def predict(self, image: Image.Image) -> AnomalyResult:\n",
    "        if self.scorer is None:\n",
    "            raise ValueError(\"Detector not trained. Call fit() first.\")\n",
    "        image_tensor = self.preprocessor.preprocess_single(image)\n",
    "        patch_features = self.feature_extractor.extract_features(image_tensor)  # (1, num_patches, D)\n",
    "        patch_features = patch_features[0].cpu().numpy()  # (num_patches, D)\n",
    "        result = self.scorer.score_image(\n",
    "            patch_features,\n",
    "            target_size=(self.input_size, self.input_size)\n",
    "        )\n",
    "        return result\n",
    "    def predict_from_path(self, image_path: str) -> AnomalyResult:\n",
    "        image = self.preprocessor.load_image(image_path)\n",
    "        return self.predict(image)\n",
    "    def predict_batch(self, images: List[Image.Image]) -> List[AnomalyResult]:\n",
    "        return [self.predict(img) for img in images]\n",
    "    def evaluate(self, test_images_dir: str, show_samples: int = 5) -> Dict:\n",
    "        if self.scorer is None:\n",
    "            raise ValueError(\"Detector not trained. Call fit() first.\")\n",
    "        image_paths = self.preprocessor.load_images_from_directory(test_images_dir)\n",
    "        print(f\"Found {len(image_paths)} test images\")\n",
    "        results = []\n",
    "        for img_path in tqdm(image_paths, desc=\"Testing images\"):\n",
    "            image = self.preprocessor.load_image(img_path)\n",
    "            result = self.predict(image)\n",
    "            results.append((img_path, image, result))\n",
    "        scores = [r[2].image_score for r in results]\n",
    "        anomalies = [r[2].is_anomaly for r in results]\n",
    "        metrics = {\n",
    "            'num_images': len(results),\n",
    "            'num_anomalies': sum(anomalies),\n",
    "            'anomaly_rate': sum(anomalies) / len(anomalies) if anomalies else 0,\n",
    "            'score_mean': np.mean(scores),\n",
    "            'score_std': np.std(scores),\n",
    "            'score_min': np.min(scores),\n",
    "            'score_max': np.max(scores)\n",
    "        }\n",
    "        print(f\"\\nTest Results:\")\n",
    "        print(f\"  Total images: {metrics['num_images']}\")\n",
    "        print(f\"  Detected anomalies: {metrics['num_anomalies']} ({metrics['anomaly_rate']*100:.1f}%)\")\n",
    "        print(f\"  Score range: [{metrics['score_min']:.4f}, {metrics['score_max']:.4f}]\")\n",
    "        print(f\"  Score mean ± std: {metrics['score_mean']:.4f} ± {metrics['score_std']:.4f}\")\n",
    "        if show_samples > 0:\n",
    "            print(f\"\\nShowing {show_samples} sample results...\")\n",
    "            for i, (path, image, result) in enumerate(results[:show_samples]):\n",
    "                print(f\"\\nSample {i+1}: {Path(path).name}\")\n",
    "                self.scorer.visualize_result(image, result)\n",
    "        return metrics\n",
    "    def save(self, path: str):\n",
    "        if self.feature_bank is None:\n",
    "            raise ValueError(\"Detector not trained. Call fit() first.\")\n",
    "        self.feature_bank.save(path + '_bank')\n",
    "        config = {\n",
    "            'input_size': self.input_size,\n",
    "            'scoring_method': self.scoring_method,\n",
    "            'k': self.k,\n",
    "            'threshold': self.scorer.threshold if self.scorer else None,\n",
    "            'patch_grid_size': self.scorer.patch_grid_size if self.scorer else None\n",
    "        }\n",
    "        np.savez(path + '_config.npz', **config)\n",
    "        print(f\"✓ Detector saved to {path}\")\n",
    "    def load(self, path: str):\n",
    "        feature_dim = self.feature_extractor.get_feature_dim()\n",
    "        self.feature_bank = NormalFeatureBank(\n",
    "            feature_dim=feature_dim,\n",
    "            use_gpu=self.use_gpu_faiss,\n",
    "            index_type=self.index_type\n",
    "        )\n",
    "        self.feature_bank.load(path + '_bank')\n",
    "        config = np.load(path + '_config.npz', allow_pickle=True)\n",
    "        self.input_size = int(config['input_size'])\n",
    "        self.scoring_method = str(config['scoring_method'])\n",
    "        self.k = int(config['k'])\n",
    "        threshold = config['threshold'].item()\n",
    "        patch_grid_size = tuple(config['patch_grid_size'])\n",
    "        self.scorer = AnomalyScorer(\n",
    "            feature_bank=self.feature_bank,\n",
    "            patch_grid_size=patch_grid_size,\n",
    "            method=self.scoring_method,\n",
    "            k=self.k,\n",
    "            threshold=threshold\n",
    "        )\n",
    "        print(f\"✓ Detector loaded from {path}\")\n",
    "print(\"✓ ZeroShotAnomalyDetector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad468a",
   "metadata": {},
   "source": [
    "## 7. Usage Example - Load DINOv3 ViT-L/16 with Text Encoder\n",
    "Load the DINOv3 ViT-L/16 model with text encoder for zero-shot anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead603e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv3 ViT-L/16 with text encoder and backbone weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\berko\\miniforge3\\envs\\dlproject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "REPO_DIR = r\"C:\\Users\\berko\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Deep Learning\\dinov3\"\n",
    "WEIGHTS_PATH = r\"C:\\Users\\berko\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Deep Learning\\dinov3_vitl16_dinotxt_vision_head_and_text_encoder-a442d8f5.pth\"\n",
    "BACKBONE_WEIGHTS_PATH = r\"C:\\Users\\berko\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Deep Learning\\dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth\"\n",
    "\n",
    "print(\"Loading DINOv3 ViT-L/16 with text encoder and backbone weights...\")\n",
    "\n",
    "dinov3_model, tokenizer = torch.hub.load(\n",
    "    REPO_DIR,\n",
    "    model='dinov3_vitl16_dinotxt_tet1280d20h24l',\n",
    "    source='local',\n",
    "    weights=WEIGHTS_PATH,\n",
    "    backbone_weights=BACKBONE_WEIGHTS_PATH\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c34f42",
   "metadata": {},
   "source": [
    "## 8. Zero-Shot Anomaly Detection with Text Prompts\n",
    "Use the text encoder to describe anomalies and compare image features to text features for zero-shot detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3cd415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity to 'broken': 0.0851\n",
      "Similarity to 'damaged surface': 0.1236\n",
      "Similarity to 'missing part': 0.1201\n",
      "Similarity to 'scratch': 0.0947\n",
      "Similarity to 'contamination': 0.0889\n",
      "Similarity to 'deformation': 0.0761\n",
      "Similarity to 'crack': 0.1097\n",
      "Similarity to 'discoloration': 0.1048\n",
      "Similarity to 'bent lead': 0.1209\n",
      "Similarity to 'corrosion': 0.1169\n",
      "Similarity to 'misplaced': 0.0842\n",
      "Similarity to 'transistor is not put in hole correctly': 0.2151\n",
      "Similarity to 'normal transistor': 0.1982\n",
      "Predicted class: transistor is not put in hole correctly\n"
     ]
    }
   ],
   "source": [
    "# Example text prompts describing anomalies\n",
    "anomaly_descriptions = [\n",
    "    \"broken\",\n",
    "    \"damaged surface\",\n",
    "    \"missing part\",\n",
    "    \"scratch\",\n",
    "    \"contamination\",\n",
    "    \"deformation\",\n",
    "    \"crack\",\n",
    "    \"discoloration\",\n",
    "    \"bent lead\",\n",
    "    \"corrosion\",\n",
    "    \"misplaced\",\n",
    "    \"transistor is not put in hole correctly\",\n",
    "    \"normal transistor\"\n",
    "]\n",
    "\n",
    "# Move model to device\n",
    "dinov3_model = dinov3_model.to(device)\n",
    "\n",
    "# Tokenize text prompts\n",
    "text_tokens = tokenizer.tokenize(anomaly_descriptions).to(device)\n",
    "\n",
    "# Encode text prompts using ViT-L/16 with text encoder\n",
    "with torch.no_grad():\n",
    "    text_features = dinov3_model.encode_text(text_tokens)\n",
    "    text_features = torch.nn.functional.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "# Load and preprocess test image\n",
    "test_image_path = r\"C:\\Users\\berko\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Deep Learning\\transistor\\test\\misplaced\\006.png\"\n",
    "image = Image.open(test_image_path).convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode image\n",
    "with torch.no_grad():\n",
    "    image_features = dinov3_model.encode_image(image_tensor)\n",
    "    image_features = torch.nn.functional.normalize(image_features, p=2, dim=-1)\n",
    "# Compute similarity between image and each text prompt\n",
    "similarities = (image_features @ text_features.T).squeeze().cpu().numpy()\n",
    "\n",
    "# Show results\n",
    "for desc, score in zip(anomaly_descriptions, similarities):\n",
    "    print(f\"Similarity to '{desc}': {score:.4f}\")\n",
    "\n",
    "pred_idx = int(np.argmax(similarities))\n",
    "print(f\"Predicted class: {anomaly_descriptions[pred_idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

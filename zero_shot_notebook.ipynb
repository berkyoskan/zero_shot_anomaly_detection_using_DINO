{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd07d9d8",
   "metadata": {},
   "source": [
    "# Zero-Shot Anomaly Detection using DINOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3b3b1",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a20382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import faiss\n",
    "from dataclasses import dataclass\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5445d",
   "metadata": {},
   "source": [
    "## ImagePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71e9343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ImagePreprocessor class defined\n"
     ]
    }
   ],
   "source": [
    "class ImagePreprocessor:\n",
    "    def __init__(self, input_size: int = 224, batch_size: int = 32):\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(int(input_size * 1.14)),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def load_image(self, image_path: str) -> Image.Image:\n",
    "        if isinstance(image_path, str):\n",
    "            return Image.open(image_path).convert('RGB')\n",
    "        return image_path\n",
    "    def preprocess_single(self, image: Image.Image) -> torch.Tensor:\n",
    "        return self.transform(image).unsqueeze(0)\n",
    "    def preprocess_batch(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        tensors = [self.transform(img) for img in images]\n",
    "        return torch.stack(tensors)\n",
    "    def load_images_from_directory(self, directory: str, extensions: Tuple[str, ...] = ('.png', '.jpg', '.jpeg')) -> List[str]:\n",
    "        directory_path = Path(directory)\n",
    "        image_paths = []\n",
    "        for ext in extensions:\n",
    "            image_paths.extend(directory_path.glob(f\"*{ext}\"))\n",
    "        return sorted([str(p) for p in image_paths])\n",
    "    def create_batches(self, image_paths: List[str]) -> List[Tuple[torch.Tensor, List[str]]]:\n",
    "        batches = []\n",
    "        for i in range(0, len(image_paths), self.batch_size):\n",
    "            batch_paths = image_paths[i:i + self.batch_size]\n",
    "            batch_images = [self.load_image(path) for path in batch_paths]\n",
    "            batch_tensor = self.preprocess_batch(batch_images)\n",
    "            batches.append((batch_tensor, batch_paths))\n",
    "        return batches\n",
    "print(\"✓ ImagePreprocessor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013752df",
   "metadata": {},
   "source": [
    "## FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c06a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, model, device: torch.device, l2_normalize: bool = True):\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.l2_normalize = l2_normalize\n",
    "        self.patch_size = 16\n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, images: torch.Tensor, return_cls: bool = False) -> torch.Tensor:\n",
    "        images = images.to(self.device)\n",
    "        if hasattr(self.model, 'forward_features'):\n",
    "            features = self.model.forward_features(images)\n",
    "            if isinstance(features, dict):\n",
    "                if 'x_norm_patchtokens' in features:\n",
    "                    patch_features = features['x_norm_patchtokens']\n",
    "                    cls_features = features.get('x_norm_clstoken', None)\n",
    "                elif 'x_prenorm' in features:\n",
    "                    all_tokens = features['x_prenorm']\n",
    "                    cls_features = all_tokens[:, 0, :]\n",
    "                    patch_features = all_tokens[:, 1:, :]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected dict keys: {features.keys()}\")\n",
    "            else:\n",
    "                if len(features.shape) == 3:\n",
    "                    cls_features = features[:, 0, :]\n",
    "                    patch_features = features[:, 1:, :]\n",
    "                elif len(features.shape) == 2:\n",
    "                    features = self.model.get_intermediate_layers(images, n=1, return_class_token=True)\n",
    "                    if isinstance(features, (list, tuple)):\n",
    "                        features = features[0]\n",
    "                    cls_features = features[0] if isinstance(features, (list, tuple)) else features[:, 0, :]\n",
    "                    patch_features = features[1] if isinstance(features, (list, tuple)) else features[:, 1:, :]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected feature shape: {features.shape}\")\n",
    "        else:\n",
    "            features = self.model(images)\n",
    "            if isinstance(features, dict):\n",
    "                if 'x_norm_patchtokens' in features:\n",
    "                    patch_features = features['x_norm_patchtokens']\n",
    "                    cls_features = features.get('x_norm_clstoken', None)\n",
    "                else:\n",
    "                    raise ValueError(f\"Cannot extract patches from dict: {features.keys()}\")\n",
    "            elif len(features.shape) == 2:\n",
    "                patch_features = self.model.get_intermediate_layers(images, n=1, return_class_token=False)[0]\n",
    "                cls_features = features\n",
    "            else:\n",
    "                cls_features = features[:, 0, :]\n",
    "                patch_features = features[:, 1:, :]\n",
    "        if self.l2_normalize:\n",
    "            patch_features = F.normalize(patch_features, p=2, dim=-1)\n",
    "            if return_cls and cls_features is not None:\n",
    "                cls_features = F.normalize(cls_features, p=2, dim=-1)\n",
    "        if return_cls:\n",
    "            return patch_features, cls_features\n",
    "        return patch_features\n",
    "    def get_feature_dim(self) -> int:\n",
    "        if hasattr(self.model, 'embed_dim'):\n",
    "            return self.model.embed_dim\n",
    "        elif hasattr(self.model, 'num_features'):\n",
    "            return self.model.num_features\n",
    "        elif hasattr(self.model, 'backbone'):\n",
    "            if hasattr(self.model.backbone, 'embed_dim'):\n",
    "                return self.model.backbone.embed_dim\n",
    "        return 384\n",
    "    def get_patch_grid_size(self, input_size: int = 224) -> Tuple[int, int]:\n",
    "        grid_h = grid_w = input_size // self.patch_size\n",
    "        return (grid_h, grid_w)\n",
    "    def extract_from_multiple_batches(self, batches: List[Tuple[torch.Tensor, List[str]]]) -> Dict[str, np.ndarray]:\n",
    "        feature_dict = {}\n",
    "        for batch_tensor, batch_paths in tqdm(batches):\n",
    "            patch_features = self.extract_features(batch_tensor)\n",
    "            patch_features_np = patch_features.cpu().numpy()\n",
    "            for i, path in enumerate(batch_paths):\n",
    "                feature_dict[path] = patch_features_np[i]\n",
    "        return feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b012c4",
   "metadata": {},
   "source": [
    "## NormalFeatureBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2ff6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalFeatureBank:\n",
    "    def __init__(self, feature_dim: int, use_gpu: bool = True, index_type: str = 'flat'):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.index_type = index_type\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available() and hasattr(faiss, 'StandardGpuResources')\n",
    "        self.index = self._create_index()\n",
    "        self.mean = None\n",
    "        self.cov_inv = None\n",
    "        self.features_array = None\n",
    "    def _create_index(self) -> faiss.Index:\n",
    "        if self.index_type == 'flat':\n",
    "            index = faiss.IndexFlatL2(self.feature_dim)\n",
    "        elif self.index_type == 'ivf':\n",
    "            quantizer = faiss.IndexFlatL2(self.feature_dim)\n",
    "            index = faiss.IndexIVFFlat(quantizer, self.feature_dim, 100)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown index type: {self.index_type}\")\n",
    "        if self.use_gpu:\n",
    "            try:\n",
    "                res = faiss.StandardGpuResources()\n",
    "                index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "            except Exception:\n",
    "                self.use_gpu = False\n",
    "        return index\n",
    "    def build_from_features(self, feature_dict: Dict[str, np.ndarray], compute_statistics: bool = True):\n",
    "        all_features = []\n",
    "        for features in feature_dict.values():\n",
    "            all_features.append(features)\n",
    "        self.features_array = np.vstack(all_features).astype('float32')\n",
    "        if self.index_type == 'ivf' and not self.index.is_trained:\n",
    "            self.index.train(self.features_array)\n",
    "        self.index.add(self.features_array)\n",
    "        if compute_statistics:\n",
    "            self._compute_statistics()\n",
    "    def _compute_statistics(self):\n",
    "        self.mean = np.mean(self.features_array, axis=0)\n",
    "        centered = self.features_array - self.mean\n",
    "        cov = np.cov(centered.T)\n",
    "        reg = 1e-5\n",
    "        cov = cov + reg * np.eye(self.feature_dim)\n",
    "        try:\n",
    "            self.cov_inv = np.linalg.inv(cov)\n",
    "        except np.linalg.LinAlgError:\n",
    "            self.cov_inv = np.linalg.pinv(cov)\n",
    "    def search_knn(self, query_features: np.ndarray, k: int = 1) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        query_features = query_features.astype('float32')\n",
    "        distances, indices = self.index.search(query_features, k)\n",
    "        return distances, indices\n",
    "    def compute_mahalanobis_distance(self, query_features: np.ndarray) -> np.ndarray:\n",
    "        if self.mean is None or self.cov_inv is None:\n",
    "            raise ValueError(\"Statistics not computed.\")\n",
    "        centered = query_features - self.mean\n",
    "        mahal = np.sum(centered @ self.cov_inv * centered, axis=1)\n",
    "        return np.sqrt(mahal)\n",
    "    def save(self, path: str):\n",
    "        save_dict = {\n",
    "            'features_array': self.features_array,\n",
    "            'mean': self.mean,\n",
    "            'cov_inv': self.cov_inv,\n",
    "            'feature_dim': self.feature_dim,\n",
    "            'index_type': self.index_type\n",
    "        }\n",
    "        index_path = path + '.index'\n",
    "        if self.use_gpu:\n",
    "            try:\n",
    "                cpu_index = faiss.index_gpu_to_cpu(self.index)\n",
    "                faiss.write_index(cpu_index, index_path)\n",
    "            except:\n",
    "                faiss.write_index(self.index, index_path)\n",
    "        else:\n",
    "            faiss.write_index(self.index, index_path)\n",
    "        np.savez(path + '.npz', **save_dict)\n",
    "    def load(self, path: str):\n",
    "        data = np.load(path + '.npz', allow_pickle=True)\n",
    "        self.features_array = data['features_array']\n",
    "        self.mean = data['mean']\n",
    "        self.cov_inv = data['cov_inv']\n",
    "        self.feature_dim = int(data['feature_dim'])\n",
    "        self.index_type = str(data['index_type'])\n",
    "        index_path = path + '.index'\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        if self.use_gpu:\n",
    "            try:\n",
    "                res = faiss.StandardGpuResources()\n",
    "                self.index = faiss.index_cpu_to_gpu(res, 0, self.index)\n",
    "            except Exception:\n",
    "                self.use_gpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d236868",
   "metadata": {},
   "source": [
    "## AnomalyScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbeced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AnomalyResult:\n",
    "    image_score: float\n",
    "    patch_scores: np.ndarray\n",
    "    anomaly_map: np.ndarray\n",
    "    is_anomaly: bool\n",
    "    method: str\n",
    "class AnomalyScorer:\n",
    "    def __init__(self, feature_bank: NormalFeatureBank, patch_grid_size: Tuple[int, int], method: str = 'knn', k: int = 1, threshold: Optional[float] = None):\n",
    "        self.feature_bank = feature_bank\n",
    "        self.patch_grid_size = patch_grid_size\n",
    "        self.method = method\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "    def compute_patch_scores(self, patch_features: np.ndarray) -> np.ndarray:\n",
    "        if self.method == 'knn':\n",
    "            distances, _ = self.feature_bank.search_knn(patch_features, k=self.k)\n",
    "            scores = np.mean(distances, axis=1)\n",
    "        elif self.method == 'mahalanobis':\n",
    "            scores = self.feature_bank.compute_mahalanobis_distance(patch_features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "        return scores\n",
    "    def create_anomaly_map(self, patch_scores: np.ndarray, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:\n",
    "        H, W = self.patch_grid_size\n",
    "        score_map = patch_scores.reshape(H, W)\n",
    "        anomaly_map = cv2.resize(score_map, target_size, interpolation=cv2.INTER_CUBIC)\n",
    "        return anomaly_map\n",
    "    def compute_image_score(self, patch_scores: np.ndarray, aggregation: str = 'max') -> float:\n",
    "        if aggregation == 'max':\n",
    "            return float(np.max(patch_scores))\n",
    "        elif aggregation == 'mean':\n",
    "            return float(np.mean(patch_scores))\n",
    "        elif aggregation == 'percentile_95':\n",
    "            return float(np.percentile(patch_scores, 95))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation: {aggregation}\")\n",
    "    def score_image(self, patch_features: np.ndarray, target_size: Tuple[int, int] = (224, 224), aggregation: str = 'max') -> AnomalyResult:\n",
    "        patch_scores = self.compute_patch_scores(patch_features)\n",
    "        image_score = self.compute_image_score(patch_scores, aggregation)\n",
    "        anomaly_map = self.create_anomaly_map(patch_scores, target_size)\n",
    "        is_anomaly = False\n",
    "        if self.threshold is not None:\n",
    "            is_anomaly = image_score > self.threshold\n",
    "        return AnomalyResult(image_score=image_score, patch_scores=patch_scores, anomaly_map=anomaly_map, is_anomaly=is_anomaly, method=self.method)\n",
    "    def set_threshold_from_normal_scores(self, normal_image_scores: List[float], percentile: float = 99.0):\n",
    "        self.threshold = np.percentile(normal_image_scores, percentile)\n",
    "    def visualize_result(self, image: Image.Image, result: AnomalyResult, figsize: Tuple[int, int] = (15, 5)):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        axes[0].imshow(image)\n",
    "        axes[1].imshow(result.anomaly_map, cmap='jet')\n",
    "        plt.colorbar(axes[1].images[0], ax=axes[1])\n",
    "        axes[2].imshow(image)\n",
    "        axes[2].imshow(result.anomaly_map, cmap='jet', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b11a8d",
   "metadata": {},
   "source": [
    "## ZeroShotAnomalyDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36991b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotAnomalyDetector:\n",
    "    def __init__(self, model, input_size: int = 224, batch_size: int = 32, device: torch.device = None, l2_normalize: bool = True, use_gpu_faiss: bool = True, index_type: str = 'flat', scoring_method: str = 'knn', k: int = 1):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preprocessor = ImagePreprocessor(input_size=input_size, batch_size=batch_size)\n",
    "        self.feature_extractor = FeatureExtractor(model, device=self.device, l2_normalize=l2_normalize)\n",
    "        self.input_size = input_size\n",
    "        self.scoring_method = scoring_method\n",
    "        self.k = k\n",
    "        self.use_gpu_faiss = use_gpu_faiss\n",
    "        self.index_type = index_type\n",
    "        self.feature_bank = None\n",
    "        self.scorer = None\n",
    "    def fit(self, normal_images_dir: str, compute_mahalanobis: bool = True):\n",
    "        image_paths = self.preprocessor.load_images_from_directory(normal_images_dir)\n",
    "        batches = self.preprocessor.create_batches(image_paths)\n",
    "        feature_dict = self.feature_extractor.extract_from_multiple_batches(batches)\n",
    "        feature_dim = self.feature_extractor.get_feature_dim()\n",
    "        self.feature_bank = NormalFeatureBank(feature_dim=feature_dim, use_gpu=self.use_gpu_faiss, index_type=self.index_type)\n",
    "        compute_stats = compute_mahalanobis or (self.scoring_method == 'mahalanobis')\n",
    "        self.feature_bank.build_from_features(feature_dict, compute_statistics=compute_stats)\n",
    "        patch_grid_size = self.feature_extractor.get_patch_grid_size(self.input_size)\n",
    "        self.scorer = AnomalyScorer(feature_bank=self.feature_bank, patch_grid_size=patch_grid_size, method=self.scoring_method, k=self.k)\n",
    "        return self\n",
    "    def set_threshold(self, validation_images_dir: str, percentile: float = 99.0):\n",
    "        validation_scores = []\n",
    "        image_paths = self.preprocessor.load_images_from_directory(validation_images_dir)\n",
    "        for img_path in image_paths:\n",
    "            image = self.preprocessor.load_image(img_path)\n",
    "            result = self.predict(image)\n",
    "            validation_scores.append(result.image_score)\n",
    "        self.scorer.set_threshold_from_normal_scores(validation_scores, percentile)\n",
    "        return self\n",
    "    def predict(self, image: Image.Image) -> AnomalyResult:\n",
    "        image_tensor = self.preprocessor.preprocess_single(image)\n",
    "        patch_features = self.feature_extractor.extract_features(image_tensor)\n",
    "        patch_features = patch_features[0].cpu().numpy()\n",
    "        result = self.scorer.score_image(patch_features, target_size=(self.input_size, self.input_size))\n",
    "        return result\n",
    "    def predict_from_path(self, image_path: str) -> AnomalyResult:\n",
    "        image = self.preprocessor.load_image(image_path)\n",
    "        return self.predict(image)\n",
    "    def predict_batch(self, images: List[Image.Image]) -> List[AnomalyResult]:\n",
    "        return [self.predict(img) for img in images]\n",
    "    def evaluate(self, test_images_dir: str, show_samples: int = 5) -> Dict:\n",
    "        image_paths = self.preprocessor.load_images_from_directory(test_images_dir)\n",
    "        results = []\n",
    "        for img_path in image_paths:\n",
    "            image = self.preprocessor.load_image(img_path)\n",
    "            result = self.predict(image)\n",
    "            results.append((img_path, image, result))\n",
    "        scores = [r[2].image_score for r in results]\n",
    "        anomalies = [r[2].is_anomaly for r in results]\n",
    "        metrics = {\n",
    "            'num_images': len(results),\n",
    "            'num_anomalies': sum(anomalies),\n",
    "            'anomaly_rate': sum(anomalies) / len(anomalies) if anomalies else 0,\n",
    "            'score_mean': np.mean(scores),\n",
    "            'score_std': np.std(scores),\n",
    "            'score_min': np.min(scores),\n",
    "            'score_max': np.max(scores)\n",
    "        }\n",
    "        return metrics\n",
    "    def save(self, path: str):\n",
    "        self.feature_bank.save(path + '_bank')\n",
    "        config = {\n",
    "            'input_size': self.input_size,\n",
    "            'scoring_method': self.scoring_method,\n",
    "            'k': self.k,\n",
    "            'threshold': self.scorer.threshold if self.scorer else None,\n",
    "            'patch_grid_size': self.scorer.patch_grid_size if self.scorer else None\n",
    "        }\n",
    "        np.savez(path + '_config.npz', **config)\n",
    "    def load(self, path: str):\n",
    "        feature_dim = self.feature_extractor.get_feature_dim()\n",
    "        self.feature_bank = NormalFeatureBank(feature_dim=feature_dim, use_gpu=self.use_gpu_faiss, index_type=self.index_type)\n",
    "        self.feature_bank.load(path + '_bank')\n",
    "        config = np.load(path + '_config.npz', allow_pickle=True)\n",
    "        self.input_size = int(config['input_size'])\n",
    "        self.scoring_method = str(config['scoring_method'])\n",
    "        self.k = int(config['k'])\n",
    "        threshold = config['threshold'].item()\n",
    "        patch_grid_size = tuple(config['patch_grid_size'])\n",
    "        self.scorer = AnomalyScorer(feature_bank=self.feature_bank, patch_grid_size=patch_grid_size, method=self.scoring_method, k=self.k, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad468a",
   "metadata": {},
   "source": [
    "## Load DINOv3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead603e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv3 ViT-L/16 with text encoder...\n",
      "✓ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "REPO_DIR = r\"C:\\Users\\berko\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Deep Learning\\dinov3\"\n",
    "WEIGHTS_PATH = r\"C:\\Users\\berko\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Deep Learning\\dinov3_vitl16_dinotxt_vision_head_and_text_encoder-a442d8f5.pth\"\n",
    "BACKBONE_WEIGHTS_PATH = r\"C:\\Users\\berko\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Deep Learning\\dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth\"\n",
    "\n",
    "print(\"Loading DINOv3 ViT-L/16 with text encoder...\")\n",
    "\n",
    "dinov3_model, tokenizer = torch.hub.load(\n",
    "    REPO_DIR,\n",
    "    model='dinov3_vitl16_dinotxt_tet1280d20h24l',\n",
    "    source='local',\n",
    "    weights=WEIGHTS_PATH,\n",
    "    backbone_weights=BACKBONE_WEIGHTS_PATH\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c34f42",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification with Text Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3cd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_THRESHOLD = 0.20\n",
    "BELOW_THRESHOLD_PENALTY = -5\n",
    "\n",
    "anomaly_descriptions = {\n",
    "    'NORMAL': [\n",
    "        \"Transistor with three long complete legs straight vertical inserted in holes\",\n",
    "        \"Transistor properly aligned upright body centered legs perpendicular\",\n",
    "        \"Perfect transistor intact no damage no bent legs clean\",\n",
    "        \"Transistor all three legs fully inserted flush contact with holes\"\n",
    "    ],\n",
    "    'BENT_LEADS': [\n",
    "        \"Transistor with legs bent twisted deformed at sharp kink angles\",\n",
    "        \"Transistor legs crooked curved irregular shapes not straight not parallel\",\n",
    "        \"Transistor severely deformed bent kinked legs uneven geometry\",\n",
    "        \"Transistor legs twisted bent outward different angles not vertical\"\n",
    "    ],\n",
    "    'DAMAGED_CASE': [\n",
    "        \"Transistor plastic body cracked fractured split shattered broken case\",\n",
    "        \"Transistor case missing chunks pieces jagged rough edges destroyed\",\n",
    "        \"Transistor body damaged destroyed deteriorated cracks throughout\",\n",
    "        \"Transistor plastic shattered broken chunks missing holes in case\"\n",
    "    ],\n",
    "    'CUT_LEADS': [\n",
    "        \"Transistor legs cut close tobody trimmed shortened not full length\",\n",
    "        \"Transistor legs fractured broken off severed at where it is close to the balck body missing length\",\n",
    "        \"Transistor has a mismatching leg pattern one or more legs cut short\",\n",
    "        \"Transistor legs do not reach holes too short cannot insert contact\"\n",
    "    ],\n",
    "    'MISPLACED': [\n",
    "        \"Transistor body rotated tilted at severe angle sideways position\",\n",
    "        \"Transistor leaning over lying horizontal not standing vertical upright\",\n",
    "        \"Transistor oriented at extreme angle rotated away not perpendicular\",\n",
    "        \"Transistor is placed horizontally\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "all_descriptions = []\n",
    "class_mapping = []\n",
    "class_names = ['NORMAL', 'BENT_LEADS', 'DAMAGED_CASE', 'CUT_LEADS', 'MISPLACED']\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    for desc in anomaly_descriptions[class_name]:\n",
    "        all_descriptions.append(desc)\n",
    "        class_mapping.append(class_idx)\n",
    "\n",
    "dinov3_model = dinov3_model.to(device)\n",
    "text_tokens = tokenizer.tokenize(all_descriptions).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = dinov3_model.encode_text(text_tokens)\n",
    "    text_features = torch.nn.functional.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "test_image_path = r\"C:\\Users\\berko\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Deep Learning\\transistor\\test\\misplaced\\005.png\"\n",
    "image = Image.open(test_image_path).convert('RGB')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = dinov3_model.encode_image(image_tensor)\n",
    "    image_features = torch.nn.functional.normalize(image_features, p=2, dim=-1)\n",
    "\n",
    "similarities = (image_features @ text_features.T).squeeze().cpu().numpy()\n",
    "similarities_squared = np.square(similarities)\n",
    "\n",
    "class_scores = {}\n",
    "for class_idx in range(len(class_names)):\n",
    "    class_desc_indices = [i for i, c in enumerate(class_mapping) if c == class_idx]\n",
    "    class_desc_scores_original = [similarities[i] for i in class_desc_indices]\n",
    "    class_desc_scores_squared = [similarities_squared[i] for i in class_desc_indices]\n",
    "    \n",
    "    penalized_scores = []\n",
    "    for orig_score, sq_score in zip(class_desc_scores_original, class_desc_scores_squared):\n",
    "        if orig_score < SCORE_THRESHOLD:\n",
    "            penalized_score = sq_score * BELOW_THRESHOLD_PENALTY\n",
    "        elif orig_score > SCORE_THRESHOLD:\n",
    "            penalized_score = sq_score * (1 + (orig_score-SCORE_THRESHOLD) * 5)\n",
    "        else:\n",
    "            penalized_score = sq_score\n",
    "        penalized_scores.append(penalized_score)\n",
    "    \n",
    "    if penalized_scores:\n",
    "        weights = np.array(penalized_scores) / np.sum(penalized_scores)\n",
    "        weighted_score = np.sum(np.array(penalized_scores) * weights)\n",
    "        class_scores[class_names[class_idx]] = weighted_score\n",
    "    else:\n",
    "        class_scores[class_names[class_idx]] = 0.0\n",
    "\n",
    "pred_class = max(class_scores.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8168ef",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa283d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 000.png | True: BENT_LEAD | Predicted: NORMAL | WRONG | Time: 0.935s\n",
      "Image: 001.png | True: BENT_LEAD | Predicted: NORMAL | WRONG | Time: 0.946s\n",
      "Image: 002.png | True: BENT_LEAD | Predicted: NORMAL | WRONG | Time: 0.930s\n",
      "Image: 003.png | True: BENT_LEAD | Predicted: NORMAL | WRONG | Time: 1.027s\n",
      "Image: 004.png | True: BENT_LEAD | Predicted: NORMAL | WRONG | Time: 0.930s\n",
      "Image: 005.png | True: BENT_LEAD | Predicted: NORMAL | WRONG | Time: 0.935s\n",
      "Image: 006.png | True: BENT_LEAD | Predicted: NORMAL | WRONG | Time: 0.926s\n",
      "Image: 007.png | True: BENT_LEAD | Predicted: CUT_LEAD | WRONG | Time: 0.933s\n",
      "Image: 008.png | True: BENT_LEAD | Predicted: NORMAL | WRONG | Time: 0.921s\n",
      "Image: 009.png | True: BENT_LEAD | Predicted: NORMAL | WRONG | Time: 0.945s\n",
      "Image: 000.png | True: CUT_LEAD | Predicted: NORMAL | WRONG | Time: 0.949s\n",
      "Image: 001.png | True: CUT_LEAD | Predicted: NORMAL | WRONG | Time: 0.937s\n",
      "Image: 002.png | True: CUT_LEAD | Predicted: NORMAL | WRONG | Time: 0.926s\n",
      "Image: 003.png | True: CUT_LEAD | Predicted: NORMAL | WRONG | Time: 0.947s\n",
      "Image: 004.png | True: CUT_LEAD | Predicted: CUT_LEAD | CORRECT | Time: 0.932s\n",
      "Image: 005.png | True: CUT_LEAD | Predicted: NORMAL | WRONG | Time: 0.952s\n",
      "Image: 006.png | True: CUT_LEAD | Predicted: NORMAL | WRONG | Time: 0.922s\n",
      "Image: 007.png | True: CUT_LEAD | Predicted: NORMAL | WRONG | Time: 0.953s\n",
      "Image: 008.png | True: CUT_LEAD | Predicted: NORMAL | WRONG | Time: 0.931s\n",
      "Image: 009.png | True: CUT_LEAD | Predicted: CUT_LEAD | CORRECT | Time: 0.936s\n",
      "Image: 000.png | True: DAMAGED_CASE | Predicted: NORMAL | WRONG | Time: 0.934s\n",
      "Image: 001.png | True: DAMAGED_CASE | Predicted: CUT_LEAD | WRONG | Time: 0.950s\n",
      "Image: 002.png | True: DAMAGED_CASE | Predicted: NORMAL | WRONG | Time: 0.934s\n",
      "Image: 003.png | True: DAMAGED_CASE | Predicted: NORMAL | WRONG | Time: 0.942s\n",
      "Image: 004.png | True: DAMAGED_CASE | Predicted: CUT_LEAD | WRONG | Time: 0.928s\n",
      "Image: 005.png | True: DAMAGED_CASE | Predicted: NORMAL | WRONG | Time: 0.939s\n",
      "Image: 006.png | True: DAMAGED_CASE | Predicted: CUT_LEAD | WRONG | Time: 0.944s\n",
      "Image: 007.png | True: DAMAGED_CASE | Predicted: CUT_LEAD | WRONG | Time: 0.946s\n",
      "Image: 008.png | True: DAMAGED_CASE | Predicted: CUT_LEAD | WRONG | Time: 0.928s\n",
      "Image: 009.png | True: DAMAGED_CASE | Predicted: CUT_LEAD | WRONG | Time: 0.933s\n",
      "Image: 000.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.927s\n",
      "Image: 001.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.947s\n",
      "Image: 002.png | True: NORMAL | Predicted: CUT_LEAD | WRONG | Time: 0.932s\n",
      "Image: 003.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.941s\n",
      "Image: 004.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.934s\n",
      "Image: 005.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.943s\n",
      "Image: 006.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.926s\n",
      "Image: 007.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.961s\n",
      "Image: 008.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.924s\n",
      "Image: 009.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.942s\n",
      "Image: 010.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.944s\n",
      "Image: 011.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.930s\n",
      "Image: 012.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.930s\n",
      "Image: 013.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.939s\n",
      "Image: 014.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.938s\n",
      "Image: 015.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.941s\n",
      "Image: 016.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.921s\n",
      "Image: 017.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.943s\n",
      "Image: 018.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.935s\n",
      "Image: 019.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.943s\n",
      "Image: 020.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.938s\n",
      "Image: 021.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.937s\n",
      "Image: 022.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.928s\n",
      "Image: 023.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.938s\n",
      "Image: 024.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.933s\n",
      "Image: 025.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.935s\n",
      "Image: 026.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.937s\n",
      "Image: 027.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.950s\n",
      "Image: 028.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.920s\n",
      "Image: 029.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.930s\n",
      "Image: 030.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.937s\n",
      "Image: 031.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.937s\n",
      "Image: 032.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 1.061s\n",
      "Image: 033.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.957s\n",
      "Image: 034.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.937s\n",
      "Image: 035.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.943s\n",
      "Image: 036.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.933s\n",
      "Image: 037.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.940s\n",
      "Image: 038.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.932s\n",
      "Image: 039.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.943s\n",
      "Image: 040.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.929s\n",
      "Image: 041.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.941s\n",
      "Image: 042.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.933s\n",
      "Image: 043.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.937s\n",
      "Image: 044.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.935s\n",
      "Image: 045.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.944s\n",
      "Image: 046.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.923s\n",
      "Image: 047.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.942s\n",
      "Image: 048.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.942s\n",
      "Image: 049.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.955s\n",
      "Image: 050.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.927s\n",
      "Image: 051.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.950s\n",
      "Image: 052.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.930s\n",
      "Image: 053.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.929s\n",
      "Image: 054.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.920s\n",
      "Image: 055.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.947s\n",
      "Image: 056.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.936s\n",
      "Image: 057.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.957s\n",
      "Image: 058.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.933s\n",
      "Image: 059.png | True: NORMAL | Predicted: NORMAL | CORRECT | Time: 0.936s\n",
      "Image: 000.png | True: MISPLACED | Predicted: NORMAL | WRONG | Time: 0.929s\n",
      "Image: 001.png | True: MISPLACED | Predicted: CUT_LEAD | WRONG | Time: 0.942s\n",
      "Image: 002.png | True: MISPLACED | Predicted: DAMAGED_CASE | WRONG | Time: 0.923s\n",
      "Image: 003.png | True: MISPLACED | Predicted: DAMAGED_CASE | WRONG | Time: 0.944s\n",
      "Image: 004.png | True: MISPLACED | Predicted: DAMAGED_CASE | WRONG | Time: 0.933s\n",
      "Image: 005.png | True: MISPLACED | Predicted: CUT_LEAD | WRONG | Time: 0.948s\n",
      "Image: 006.png | True: MISPLACED | Predicted: NORMAL | WRONG | Time: 0.925s\n",
      "Image: 007.png | True: MISPLACED | Predicted: NORMAL | WRONG | Time: 0.940s\n",
      "Image: 008.png | True: MISPLACED | Predicted: CUT_LEAD | WRONG | Time: 0.933s\n",
      "Image: 009.png | True: MISPLACED | Predicted: NORMAL | WRONG | Time: 0.942s\n",
      "\n",
      "Classification Accuracy by Class:\n",
      "NORMAL         : 59/60 (98.33%)\n",
      "BENT_LEAD      : 0/10 (0.00%)\n",
      "DAMAGED_CASE   : 0/10 (0.00%)\n",
      "CUT_LEAD       : 2/10 (20.00%)\n",
      "MISPLACED      : 0/10 (0.00%)\n",
      "\n",
      "Summary Table:\n",
      "---------------------------------------------------------------\n",
      "Overall classification accuracy (specific): 61/100 (61.00%)\n",
      "General anomaly/normal accuracy:           74/100 (74.00%)\n",
      "Penalized accuracy (false positives):      75/100 (75.00%)\n",
      "Average time per image:                    0.976 seconds\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE_TEST_PATH = r\"C:\\Users\\berko\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Deep Learning\\transistor\\test\"\n",
    "\n",
    "folder_to_class = {\n",
    "    'good': 'NORMAL',\n",
    "    'bent_lead': 'BENT_LEAD',\n",
    "    'damaged_case': 'DAMAGED_CASE',\n",
    "    'cut_lead': 'CUT_LEAD',\n",
    "    'misplaced': 'MISPLACED'\n",
    "}\n",
    "\n",
    "class_names = ['NORMAL', 'BENT_LEAD', 'DAMAGED_CASE', 'CUT_LEAD', 'MISPLACED']\n",
    "\n",
    "results_by_class = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "all_correct = 0\n",
    "all_total = 0\n",
    "general_correct = 0\n",
    "general_total = 0\n",
    "penalized_correct = 0\n",
    "penalized_total = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for folder in os.listdir(BASE_TEST_PATH):\n",
    "    folder_path = os.path.join(BASE_TEST_PATH, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    class_label = folder_to_class.get(folder)\n",
    "    if class_label is None:\n",
    "        print(f\"Skipping folder '{folder}' (not mapped to class)\")\n",
    "        continue\n",
    "    \n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.lower().endswith('.png'):\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(folder_path, fname)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            image_features = dinov3_model.encode_image(image_tensor)\n",
    "            image_features = torch.nn.functional.normalize(image_features, p=2, dim=-1)\n",
    "            similarities = (image_features @ text_features.T).squeeze().cpu().numpy()\n",
    "        \n",
    "        similarities_squared = np.square(similarities)\n",
    "        \n",
    "        class_scores = {}\n",
    "        for class_idx in range(len(class_names)):\n",
    "            class_desc_indices = [i for i, c in enumerate(class_mapping) if c == class_idx]\n",
    "            class_desc_scores_original = [similarities[i] for i in class_desc_indices]\n",
    "            class_desc_scores_squared = [similarities_squared[i] for i in class_desc_indices]\n",
    "            \n",
    "            penalized_scores = []\n",
    "            for orig_score, sq_score in zip(class_desc_scores_original, class_desc_scores_squared):\n",
    "                if orig_score < SCORE_THRESHOLD:\n",
    "                    penalized_score = sq_score * BELOW_THRESHOLD_PENALTY\n",
    "                elif orig_score > SCORE_THRESHOLD:\n",
    "                    penalized_score = sq_score * (1 + (orig_score-SCORE_THRESHOLD) * 5)\n",
    "                else:\n",
    "                    penalized_score = sq_score\n",
    "                penalized_scores.append(penalized_score)\n",
    "            \n",
    "            if penalized_scores:\n",
    "                weights = np.array(penalized_scores) / np.sum(penalized_scores)\n",
    "                weighted_score = np.sum(np.array(penalized_scores) * weights)\n",
    "                class_scores[class_names[class_idx]] = weighted_score\n",
    "            else:\n",
    "                class_scores[class_names[class_idx]] = 0.0\n",
    "        \n",
    "        pred_class = max(class_scores.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        results_by_class[class_label]['total'] += 1\n",
    "        all_total += 1\n",
    "        \n",
    "        if pred_class == class_label:\n",
    "            results_by_class[class_label]['correct'] += 1\n",
    "            all_correct += 1\n",
    "        \n",
    "        is_true_normal = (class_label == 'NORMAL')\n",
    "        is_pred_normal = (pred_class == 'NORMAL')\n",
    "        is_true_anomaly = not is_true_normal\n",
    "        is_pred_anomaly = not is_pred_normal\n",
    "        \n",
    "        if (is_true_normal and is_pred_normal) or (is_true_anomaly and is_pred_anomaly):\n",
    "            general_correct += 1\n",
    "        general_total += 1\n",
    "        \n",
    "        penalized_total += 1\n",
    "        if pred_class == class_label:\n",
    "            penalized_correct += 1\n",
    "        elif is_true_anomaly and is_pred_normal:\n",
    "            pass\n",
    "        elif is_true_normal and is_pred_anomaly:\n",
    "            penalized_correct += 1\n",
    "        elif is_true_anomaly and is_pred_anomaly:\n",
    "            penalized_correct += 1\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"Image: {fname} | True: {class_label} | Predicted: {pred_class} | {'CORRECT' if pred_class == class_label else 'WRONG'} | Time: {elapsed:.3f}s\")\n",
    "\n",
    "end_time = time.time()\n",
    "avg_time = (end_time - start_time) / all_total if all_total > 0 else 0.0\n",
    "\n",
    "print(\"\\nClassification Accuracy by Class:\")\n",
    "for class_label in class_names:\n",
    "    correct = results_by_class[class_label]['correct']\n",
    "    total = results_by_class[class_label]['total']\n",
    "    acc = (correct / total * 100) if total > 0 else 0.0\n",
    "    print(f\"{class_label:15s}: {correct}/{total} ({acc:.2f}%)\")\n",
    "\n",
    "overall_acc = (all_correct / all_total * 100) if all_total > 0 else 0.0\n",
    "general_acc = (general_correct / general_total * 100) if general_total > 0 else 0.0\n",
    "penalized_acc = (penalized_correct / penalized_total * 100) if penalized_total > 0 else 0.0\n",
    "\n",
    "print(\"\\nSummary Table:\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(f\"Overall classification accuracy (specific): {all_correct}/{all_total} ({overall_acc:.2f}%)\")\n",
    "print(f\"General anomaly/normal accuracy:           {general_correct}/{general_total} ({general_acc:.2f}%)\")\n",
    "print(f\"Penalized accuracy (false positives):      {penalized_correct}/{penalized_total} ({penalized_acc:.2f}%)\")\n",
    "print(f\"Average time per image:                    {avg_time:.3f} seconds\")\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
